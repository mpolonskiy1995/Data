{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Envs\\UKSH\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from random import randrange\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchmetrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n",
    "device = torch.device(dev) \n",
    "\n",
    "size = (150,150)\n",
    "\n",
    "path = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "### Calculating mean and std of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "convert = transforms.Compose([   \n",
    "\n",
    "    transforms.Grayscale(),\n",
    "    # resize\n",
    "    transforms.Resize(size),\n",
    "    # to-tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    \n",
    "])\n",
    "\n",
    "for filename in os.listdir(\"testimages\"):\n",
    "    img = Image.open(f\"testimages/{filename}\").convert('RGB')\n",
    "    img = convert(img)\n",
    "    imgs.append(img)\n",
    "imgs = torch.stack(imgs)\n",
    "\n",
    "imgs_mean = imgs.mean()\n",
    "imgs_std = imgs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9898), tensor(0.0786))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_mean, imgs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Grayscale(),\n",
    "    # resize\n",
    "    transforms.Resize(size),\n",
    "    # to-tensor\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "    transforms.Normalize((imgs_mean), (imgs_std))\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimages():\n",
    "    images = []\n",
    "    for i in range(0,500):\n",
    "        img = Image.open(f\"testimages/img{i}.png\").convert('RGB')\n",
    "        img = transform(img)\n",
    "        images.append(img)\n",
    "    images = torch.stack(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = loadimages()\n",
    "pairs = np.load(\"imagePairs.npy\", allow_pickle=True ).astype(int)\n",
    "labels = np.load(\"imageLabels.npy\", allow_pickle=True ).astype(float)\n",
    "\n",
    "pairs = pairs[500:24000]\n",
    "labels = labels[500:24000]\n",
    "\n",
    "# Split into Train, Val and Test\n",
    "rndidx = np.random.choice(np.arange(0,  len(pairs)), size = len(pairs) )\n",
    "trainidx, remain = np.array_split(rndidx, [int(0.7 * len(rndidx))])\n",
    "validx, testidx = np.array_split(remain, [int(0.5 * len(remain))])\n",
    "\n",
    "trainpairs = pairs[trainidx]\n",
    "valpairs = pairs[validx]\n",
    "testpairs = pairs[testidx]\n",
    "\n",
    "trainlabels = labels[trainidx]\n",
    "vallabels = labels[validx]\n",
    "testlabels = labels[testidx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(8,5))\n",
    "sns.histplot(trainlabels, ax=ax[0])\n",
    "ax[0].set_title(\"Train Label Dist\")\n",
    "sns.histplot(vallabels, ax=ax[1])\n",
    "ax[1].set_title(\"Val Label Dist\")\n",
    "sns.histplot(testlabels, ax=ax[2])\n",
    "ax[2].set_title(\"Test Label Dist\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HazelNet(nn.Module):\n",
    "    \"\"\"Class for instanciating the NN\n",
    "\n",
    "    Args:\n",
    "        nn (nn.Module): super class to inherit from in pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Cosntructor for initialization\n",
    "        \"\"\"\n",
    "        super(HazelNet, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "  \n",
    "        # over-write the first conv layer to be able to read images\n",
    "        # as resnet18 reads (3,x,x) where 3 is RGB channels\n",
    "        # whereas MNIST has (1,x,x) where 1 is a gray-scale channel\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        self.fc_in_features = self.resnet.fc.in_features \n",
    "        # remove the last layer of resnet18 (linear layer which is before avgpool layer)\n",
    "        self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "\n",
    "        # add linear layers to compare between the features of the two images\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_in_features, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.Linear(1024, 128),\n",
    "            )        \n",
    "       \n",
    "        # initialize the weights\n",
    "        self.resnet.apply(self.init_weights)\n",
    "        self.fc.apply(self.init_weights)   \n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        \"\"\"Function for weight init\n",
    "\n",
    "        Args:\n",
    "            m (module): module to use for init\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward_once(self, inputs):\n",
    "        \"\"\"Helper function for forward path\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            tensor: output tensor\n",
    "        \"\"\"\n",
    "        output = self.resnet(inputs)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def distance_layer(self, vec1, vec2):\n",
    "        \"\"\"Function for calculating the cosine similarity between two tensors\n",
    "\n",
    "        Args:\n",
    "            vec1 (tensor): tensor for template images\n",
    "            vec2 (tensor): tensor for images to compare with\n",
    "\n",
    "        Returns:\n",
    "            tensor: tensor containing the calculated similarity as float\n",
    "        \"\"\"\n",
    "        cos = torch.nn.CosineSimilarity()\n",
    "        similarity = cos(vec1, vec2) \n",
    "        return similarity\n",
    "\n",
    "    def forward(self, template, img):\n",
    "        \"\"\"Main function for forward path\n",
    "\n",
    "        Args:\n",
    "            template (tensor): tensor of template images\n",
    "            img (tensor): tensor of images to compare\n",
    "\n",
    "        Returns:\n",
    "            tensor: tensor containing the calculated similarity as float\n",
    "        \"\"\"\n",
    "        output1 = self.forward_once(template)\n",
    "        output2 = self.forward_once(img)\n",
    "        output = self.distance_layer(output1,output2)\n",
    " \n",
    "        return output\n",
    "\n",
    "    def readImg_url (self, url1, url2, iswhite = False, plot = False):\n",
    "        \"\"\"Function for reading images into processable tensors. Can draw a picture of processed images\n",
    "\n",
    "        Args:\n",
    "            url1 (string): url to template image\n",
    "            url2 (string): url to image for comparison\n",
    "            iswhite (bool, optional): inverts image colors if set to true. Defaults to False.\n",
    "            plot (bool, optional): plots imported images if set to true. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tensor, tensor: two tensors containing the processed images ready for prediction\n",
    "        \"\"\"\n",
    "        # invert white and black if image is on white background\n",
    "        if iswhite:\n",
    "            realim1 = cv.bitwise_not(cv.imread(url1,0)).astype(np.float32)\n",
    "            realim2 = cv.bitwise_not(cv.imread(url2,0)).astype(np.float32)\n",
    "        else: \n",
    "            realim1 = cv.imread(url1,0).astype(np.float32)\n",
    "            realim2 = cv.imread(url2,0).astype(np.float32)\n",
    "\n",
    "        realim1 =  cv.resize(realim1, imgsize)\n",
    "        realim2 =  cv.resize(realim2, imgsize)\n",
    "        template = torch.tensor(realim1).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        img = torch.tensor(realim2.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1,2)\n",
    "            ax[0].imshow(realim1)\n",
    "            ax[1].imshow(realim2)\n",
    "        return template, img        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for creating a Dataset which the dataloader from pytroch needs to create a wrapper for iterating though the data\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root, pairs, labels, images):\n",
    "       self.pairs = pairs\n",
    "       self.labels = labels\n",
    "       self.images = images\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        element = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "        template = self.images[self.pairs[idx][0]]\n",
    "        img = self.images[self.pairs[idx][1]]\n",
    "        img = augmenter(img)\n",
    "        img = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2))(img) if randrange(4) == 1 else img\n",
    "\n",
    "        return  template, img , label.astype(np.float32) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40 if torch.cuda.is_available() else 64\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 100\n",
    "\n",
    "loss_history = []\n",
    "r2_history = []\n",
    "\n",
    "\n",
    "r2score = torchmetrics.R2Score().to(device)\n",
    "mape = torchmetrics.MeanAbsolutePercentageError().to(device)\n",
    "mse = torchmetrics.MeanSquaredError().to(device)\n",
    "mae = torchmetrics.MeanAbsoluteError().to(device)\n",
    "\n",
    "metrics = {\"loss\": 0 ,\"r2\" : r2score, \"mse\" : mse, \"mae\" : mae, \"mape\" : mape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HazelNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_data = CustomImageDataset(\"\", pairs=trainpairs, labels=trainlabels, images=images)\n",
    "val_data = CustomImageDataset(\"\",  pairs=valpairs, labels=vallabels, images=images)\n",
    "test_data = CustomImageDataset(\"\", pairs=testpairs, labels=testlabels, images=images)\n",
    "\n",
    "# Create data loaders.\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, train_loader):\n",
    "    \"\"\"Function for training a NN for one epoch\n",
    "\n",
    "    Args:\n",
    "        epoch_index (int): index of current epoch\n",
    "        train_loader (data loader): loader containing training data\n",
    "\n",
    "    Returns:\n",
    "        float, tensor: averaged loss and r2 coefficient\n",
    "    \"\"\"\n",
    "\n",
    "    running = {\"loss\" :0, \"r2\" : 0, \"mse\" : 0, \"mae\" : 0, \"mape\" : 0}\n",
    "    total = {\"loss\" : 0, \"r2\" : 0, \"mse\" : 0, \"mae\" : 0, \"mape\" : 0}\n",
    "\n",
    "\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        templates, images, targets = data\n",
    "        \n",
    "        templates = templates.to(device)\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(templates, images)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        for key, metric in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                running[key] += loss\n",
    "                total[key] += loss\n",
    "            else:\n",
    "                running[key] += metric(outputs, torch.tensor(targets).squeeze())\n",
    "                total[key] += metric(outputs, torch.tensor(targets).squeeze())\n",
    "\n",
    "\n",
    "        if idx % 100 == 80:\n",
    "            running = {k: v / 100 for k,v in running.items()}\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.4f} R2Score: {:.4f} MSE: {:.4f} MAE {:.4f} MAPE {:.4f} '.format(\n",
    "            epoch_index, idx * len(templates), len(train_loader.dataset),\n",
    "            100. * idx / len(train_loader), running[\"loss\"], running[\"r2\"], running[\"mse\"], running[\"mae\"], running[\"mape\"]))\n",
    "            running = {\"loss\" :0, \"r2\" : 0, \"mse\" : 0, \"mae\" : 0, \"mape\" : 0}\n",
    "\n",
    "    total = {k: v / (idx + 1) for k,v in total.items()}\n",
    "\n",
    "    del templates\n",
    "    del images\n",
    "    del targets\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel (train_loader, val_loader, saveModel = False):\n",
    "    \"\"\"Main Function for training the NN\n",
    "\n",
    "    Args:\n",
    "        train_loader (data loader): loader containing training data\n",
    "        val_loader (data loader): loader containing val data\n",
    "        saveModel (bool, optional): regulates whether to save the best model. Defaults to False.\n",
    "    \"\"\"\n",
    "    log = pd.DataFrame()\n",
    "    torch.cuda.empty_cache()\n",
    "    tb = SummaryWriter()\n",
    "    best_vloss = 1000000\n",
    "\n",
    "    for epoch in range (num_epochs):\n",
    "\n",
    "        model.train(True)\n",
    "        train_avgs = train_one_epoch(epoch_index=epoch, train_loader=train_loader)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train(False)\n",
    "\n",
    "        total = {\"loss\" : 0, \"r2\" : 0, \"mse\" : 0, \"mae\" : 0, \"mape\" : 0}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, vdata in enumerate(val_loader):\n",
    "                vtemplates, vimages, vtargets = vdata\n",
    "                vtemplates = vtemplates.to(device)\n",
    "                vimages = vimages.to(device)\n",
    "                vtargets = vtargets.to(device)\n",
    "\n",
    "                voutputs = model(vtemplates,vimages)\n",
    "                vloss =  criterion(voutputs, vtargets)\n",
    "\n",
    "                for key, metric in metrics.items():\n",
    "                    if key == \"loss\":\n",
    "                        total[key] += vloss\n",
    "                    else:\n",
    "                        total[key] += metric(voutputs, torch.tensor(vtargets).squeeze())\n",
    "\n",
    "\n",
    "            val_avgs = {k: v / (idx + 1) for k,v in total.items()}\n",
    " \n",
    "            print('EPOCH RESULTS: Train Loss {:.4f} Valid Loss {:.4f} Train R2 {:.4f} Valid R2 {:.4f} Train MSE {:.4f} Val MSE {:.4f} Train MAE {:.4f} Val MAE {:.4f} Train MAPE {:.4f} Val MAPE {:.4f} '.format(\n",
    "                    train_avgs[\"loss\"], val_avgs[\"loss\"], train_avgs[\"r2\"], val_avgs[\"r2\"], train_avgs[\"mse\"], val_avgs[\"mse\"], train_avgs[\"mae\"], val_avgs[\"mae\"], train_avgs[\"mape\"], val_avgs[\"mape\"]))\n",
    "\n",
    "            tb.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : train_avgs[\"loss\"], 'Validation' : val_avgs[\"loss\"]},\n",
    "                            epoch + 1)\n",
    "\n",
    "\n",
    "            tb.add_scalars('Training vs. Validation R2',\n",
    "                            { 'Training' : train_avgs[\"r2\"], 'Validation' : val_avgs[\"r2\"]},\n",
    "                            epoch + 1)\n",
    "            tb.flush()    \n",
    "\n",
    "        log = log.append({f\"train_{k}\" : v.cpu().item()  for k,v in train_avgs.items()} | {f\"val_{k}\" : v.cpu().item()  for k,v in val_avgs.items()}, ignore_index=True)\n",
    "        # Track best performance, and save the model's state\n",
    "        if saveModel and val_avgs[\"loss\"] < best_vloss:\n",
    "             best_vloss = val_avgs[\"loss\"]\n",
    "             torch.save(model.state_dict(), f\"{path}/Model/bestmodel\")\n",
    "\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m log \u001b[39m=\u001b[39m trainModel(train_loader\u001b[39m=\u001b[39mtrain_loader, val_loader\u001b[39m=\u001b[39mval_loader)\n",
      "Cell \u001b[1;32mIn [14], line 17\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(train_loader, val_loader, saveModel)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (num_epochs):\n\u001b[0;32m     16\u001b[0m     model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m     train_avgs \u001b[39m=\u001b[39m train_one_epoch(epoch_index\u001b[39m=\u001b[39;49mepoch, train_loader\u001b[39m=\u001b[39;49mtrain_loader)\n\u001b[0;32m     19\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m     20\u001b[0m     model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [13], line 25\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, train_loader)\u001b[0m\n\u001b[0;32m     21\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[39m=\u001b[39m model(templates, images)\n\u001b[0;32m     27\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     28\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\micha\\Envs\\UKSH\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [8], line 89\u001b[0m, in \u001b[0;36mHazelNet.forward\u001b[1;34m(self, template, img)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, template, img):\n\u001b[0;32m     80\u001b[0m     \u001b[39m\"\"\"Main function for forward path\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m        tensor: tensor containing the calculated similarity as float\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(template)\n\u001b[0;32m     90\u001b[0m     output2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_once(img)\n\u001b[0;32m     91\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistance_layer(output1,output2)\n",
      "Cell \u001b[1;32mIn [8], line 60\u001b[0m, in \u001b[0;36mHazelNet.forward_once\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_once\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\"\"Helper function for forward path\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m        tensor: output tensor\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet(inputs)\n\u001b[0;32m     61\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(output\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     62\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n",
      "File \u001b[1;32mc:\\Users\\micha\\Envs\\UKSH\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\micha\\Envs\\UKSH\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\micha\\Envs\\UKSH\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\micha\\Envs\\UKSH\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:151\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    149\u001b[0m     \u001b[39m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_batches_tracked\u001b[39m.\u001b[39;49madd_(\u001b[39m1\u001b[39;49m)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    152\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    153\u001b[0m             exponential_average_factor \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log = trainModel(train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"bestmodel_area\")\n",
    "#log.to_csv(\"bestmodellog_area\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (model, template, img):\n",
    "    \"\"\"Function for predicting the correlation score between two images\n",
    "\n",
    "    Args:\n",
    "        model (HazelNet): The model to use for prediction\n",
    "        template (tensor): template image\n",
    "        img (tensor): image to compare to\n",
    "\n",
    "    Returns:\n",
    "        float: label containing predicted similarity score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ypred = model(template,img)\n",
    "    return round(ypred.item(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Grayscale(),\n",
    "    # resize\n",
    "    transforms.Resize(size),\n",
    "    # to-tensor\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "    transforms.Normalize((0.98), (0.07))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImg_url ( url1, url2, isblack = False, plot = False):\n",
    "        \"\"\"Function for reading images into processable tensors. Can draw a picture of processed images\n",
    "\n",
    "        Args:\n",
    "            url1 (string): url to template image\n",
    "            url2 (string): url to image for comparison\n",
    "            isblack (bool, optional): inverts image colors if set to true. Defaults to False.\n",
    "            plot (bool, optional): plots imported images if set to true. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tensor, tensor: two tensors containing the processed images ready for prediction\n",
    "        \"\"\"\n",
    "\n",
    "        realim1 = Image.open(url1)\n",
    "        realim2 = Image.open(url2)\n",
    "        # invert white and black if image is on white background\n",
    "        if isblack:\n",
    "            realim1 = PIL.ImageOps.invert(realim1)\n",
    "            realim2 = PIL.ImageOps.invert(realim2)\n",
    "\n",
    "        template = transform(realim1).unsqueeze(0).to(device)\n",
    "        img = transform(realim2).unsqueeze(0).to(device)\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1,2)\n",
    "            ax[0].imshow(realim1)\n",
    "            ax[1].imshow(realim2)\n",
    "        return template, img        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'c:\\\\Users\\\\micha\\\\Documents\\\\FH Kiel\\\\UKSH\\\\Abgabe Application Project'\n",
    "url1 = f\"{path}/Real Images/Peaks/Peak1.png\"\n",
    "url2 = f\"{path}/Real Images/Peaks/Peak6.png\"\n",
    "\n",
    "template, img = readImg_url(url1= url1, url2 =url2,  isblack=False, plot=True )\n",
    "predict(model = model, template = template, img = img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(test_loader))\n",
    "img1 = test[0][0].squeeze(0).numpy()\n",
    "img2 = test[1][0].squeeze(0).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)\n",
    "fig.suptitle(test[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model = model, template = test[0][0].unsqueeze(0).to(device), img = test[1][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HazelNet().to(device)\n",
    "#torch.save(model.state_dict(), f\"test\")\n",
    "model.load_state_dict(torch.load(f\"bestmodel_area\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UKSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bcfe374eb6962dfaa81be4d9965aebe0aa9e8512cbd375dd90ce444d679a914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
